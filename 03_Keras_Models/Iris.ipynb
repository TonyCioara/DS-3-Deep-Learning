{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = np_utils.to_categorical(y_train)\n",
    "y_test_one_hot = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(4,)))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train_one_hot, epochs=100, batch_size=1, verbose=0)\n",
    "loss, accuracy = model.evaluate(X_test, y_test_one_hot, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9777777791023254\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(4,))\n",
    "\n",
    "layer1 = Dense(16, activation=\"sigmoid\") (inputs)\n",
    "predictions = Dense(3, activation=\"softmax\") (layer1)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train_one_hot, epochs=100, batch_size=1, verbose=0)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test_one_hot, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11023169424798754 \n",
      " 0.9777777791023254\n"
     ]
    }
   ],
   "source": [
    "print(loss, '\\n', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Network.summary of <keras.engine.training.Model object at 0x1a172c5dd8>>\n"
     ]
    }
   ],
   "source": [
    "print(model.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.6000000052981906\n",
      "2 0.6000000052981906\n",
      "3 0.40000000794728596\n",
      "4 0.6000000052981906\n",
      "5 0.6000000052981906\n",
      "6 0.6888888955116272\n",
      "7 0.6000000052981906\n",
      "8 0.3555555588669247\n",
      "9 0.35555555754237705\n",
      "10 0.733333338631524\n",
      "11 0.35555555754237705\n",
      "12 0.2888888908757104\n",
      "13 0.2444444477558136\n",
      "14 0.4888888955116272\n",
      "15 0.2444444477558136\n",
      "16 0.2444444477558136\n",
      "17 0.35555555754237705\n",
      "18 0.35555555754237705\n",
      "19 0.15555555721124012\n",
      "20 0.3555555588669247\n",
      "21 0.7555555608537462\n",
      "22 0.7555555608537462\n",
      "23 0.2444444477558136\n",
      "24 0.2444444477558136\n",
      "25 0.2444444477558136\n",
      "26 0.26666666997803584\n",
      "27 0.5111111177338494\n",
      "28 0.2444444477558136\n",
      "29 0.2444444477558136\n",
      "30 0.7555555608537462\n",
      "31 0.35555555754237705\n",
      "32 0.26666666997803584\n",
      "33 0.2444444477558136\n",
      "34 0.2444444477558136\n",
      "35 0.13333333416117563\n",
      "36 0.35555555754237705\n",
      "37 0.2444444477558136\n",
      "38 0.35555555754237705\n",
      "39 0.2444444477558136\n",
      "40 0.35555555754237705\n",
      "41 0.4888888895511627\n",
      "42 0.35555555754237705\n",
      "43 0.4000000019868215\n",
      "44 0.40000000331136915\n",
      "45 0.40000000331136915\n",
      "46 0.7555555608537462\n",
      "47 0.40000000331136915\n",
      "48 0.35555555754237705\n",
      "49 0.5555555575423771\n",
      "50 0.40000000331136915\n",
      "51 0.35555555754237705\n",
      "52 0.26666666997803584\n",
      "53 0.35555555754237705\n",
      "54 0.35555555754237705\n",
      "55 0.35555555754237705\n",
      "56 0.35555555754237705\n",
      "57 0.35555555754237705\n",
      "58 0.2444444477558136\n",
      "59 0.48888889683617487\n",
      "60 0.40000000331136915\n",
      "61 0.35555555754237705\n",
      "62 0.40000000331136915\n",
      "63 0.40000000331136915\n",
      "64 0.35555555754237705\n",
      "65 0.0\n",
      "66 0.2444444477558136\n",
      "67 0.35555555754237705\n",
      "68 0.2444444477558136\n",
      "69 0.2444444477558136\n",
      "70 0.7555555608537462\n",
      "71 0.40000000331136915\n",
      "72 0.40000000331136915\n",
      "73 0.2444444477558136\n",
      "74 0.40000000331136915\n",
      "75 0.7111111164093018\n",
      "76 0.35555555754237705\n",
      "77 0.35555555754237705\n",
      "78 0.2444444477558136\n",
      "79 0.35555555754237705\n",
      "80 0.35555555754237705\n",
      "81 0.40000000331136915\n",
      "82 0.044444444444444446\n",
      "83 0.7555555608537462\n",
      "84 0.6000000052981906\n",
      "85 0.2444444477558136\n",
      "86 0.2444444477558136\n",
      "87 0.2444444477558136\n",
      "88 0.35555555754237705\n",
      "89 0.40000000331136915\n",
      "90 0.35555555754237705\n",
      "91 0.35555555754237705\n",
      "92 0.35555555754237705\n",
      "93 0.2444444477558136\n",
      "94 0.35555555754237705\n",
      "95 0.35555555754237705\n",
      "96 0.35555555754237705\n",
      "97 0.35555555754237705\n",
      "98 0.40000000331136915\n",
      "99 0.7555555608537462\n",
      "100 0.35555555754237705\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for batch_size in range(1, 101):\n",
    "    inputs = Input(shape=(4,))\n",
    "\n",
    "    layer1 = Dense(16, activation=\"sigmoid\") (inputs)\n",
    "    predictions = Dense(3, activation=\"softmax\") (layer1)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, y_train_one_hot, epochs=5, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_test, y_test_one_hot, verbose=0)\n",
    "    print(batch_size, accuracy)\n",
    "    results.append([batch_size, accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0.9777777791023254], [2, 0.9777777791023254], [3, 0.9777777791023254], [4, 0.9777777791023254], [5, 0.9777777791023254], [6, 0.9777777791023254], [7, 0.9777777791023254], [8, 0.9777777791023254], [9, 0.9777777791023254], [10, 0.9777777791023254], [11, 0.9777777791023254], [12, 0.9777777791023254], [13, 0.9777777791023254], [14, 0.9777777791023254], [15, 0.9777777791023254], [16, 0.9777777791023254], [17, 0.9777777791023254], [18, 0.9777777791023254], [19, 0.9777777791023254], [20, 0.9777777791023254], [21, 0.9777777791023254], [22, 0.9777777791023254], [23, 0.9777777791023254], [24, 0.9777777791023254], [25, 0.9777777791023254], [26, 0.9777777791023254], [27, 0.9777777791023254], [28, 0.9777777791023254], [29, 0.9777777791023254], [30, 0.9777777791023254], [31, 0.9777777791023254], [32, 0.9777777791023254], [33, 0.9777777791023254], [34, 0.9777777791023254], [35, 0.9777777791023254], [36, 0.9777777791023254], [37, 0.9777777791023254], [38, 0.9777777791023254], [39, 0.9777777791023254], [40, 0.9777777791023254], [41, 0.9777777791023254], [42, 0.9777777791023254], [43, 0.9777777791023254], [44, 0.9777777791023254], [45, 0.9777777791023254], [46, 0.9777777791023254], [47, 0.9777777791023254], [48, 0.9777777791023254], [49, 0.9777777791023254], [50, 0.9777777791023254], [51, 0.9777777791023254], [52, 0.9777777791023254], [53, 0.9777777791023254], [54, 0.9777777791023254], [55, 0.9777777791023254], [56, 0.9777777791023254], [57, 0.9777777791023254], [58, 0.9777777791023254], [59, 0.9777777791023254], [60, 0.9777777791023254], [61, 0.9777777791023254], [62, 0.9777777791023254], [63, 0.9777777791023254], [64, 0.9777777791023254], [65, 0.9777777791023254], [66, 0.9777777791023254], [67, 0.9777777791023254], [68, 0.9777777791023254], [69, 0.9777777791023254], [70, 0.9777777791023254], [71, 0.9777777791023254], [72, 0.9777777791023254], [73, 0.9777777791023254], [74, 0.9777777791023254], [75, 0.9777777791023254], [76, 0.9777777791023254], [77, 0.9777777791023254], [78, 0.9777777791023254], [79, 0.9777777791023254], [80, 0.9777777791023254], [81, 0.9777777791023254], [82, 0.9777777791023254], [83, 0.9777777791023254], [84, 0.9777777791023254], [85, 0.9777777791023254], [86, 0.9777777791023254], [87, 0.9777777791023254], [88, 0.9777777791023254], [89, 0.9777777791023254], [90, 0.9555555568801032], [91, 0.9777777791023254], [92, 0.9777777791023254], [93, 0.9777777791023254], [94, 0.9777777791023254], [95, 0.9777777791023254], [96, 0.9777777791023254], [97, 0.9777777791023254], [98, 0.9777777791023254], [99, 0.9777777791023254], [100, 0.9777777791023254]]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
